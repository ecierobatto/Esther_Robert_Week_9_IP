{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moringa_School_Independent_Project_Week_9_12_Esther_Robert_Naive Bayes_Python_Notebook",
      "provenance": [],
      "collapsed_sections": [
        "TFscA7tZ0LcM",
        "dS5RBcjtl-7K",
        "lBx5GgVne2iB",
        "2DFEM2E9lfOc",
        "YRQeJ3hnfRKX",
        "-fCD2btO27R-",
        "ZWEm-ye5tW-u",
        "fq3_NNDcwbKS",
        "Yfd85ds6mdOD",
        "qRHpj8hb3SzV",
        "NZ6qu16n4Gyh",
        "m2p37d5B4o9O",
        "wuMQmEkDMTNC",
        "hamVnK16J4Hr",
        "vQ-GLniZrpx0",
        "KOmTv2bUnvCO",
        "nymHHqpZsXAl"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPTn3vn90hzMI4L/ARqFUJI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecierobatto/Esther_Robert_Week_9_IP/blob/main/Moringa_School_Independent_Project_Week_9_12_Esther_Robert_Naive_Bayes_Python_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes- Exploratory Data Analysis"
      ],
      "metadata": {
        "id": "z2kE-ZrIESKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specifying the Question\n",
        "We will train a classifier to detect e-mail spam. Here are the specifications:\n",
        "\n",
        "**Objective**: predict whether each e-mail is spam or not.\n",
        "\n",
        "**Possible classes**: spam: 1, non-spam: 0\n",
        "\n",
        "**Features**: all 57 features"
      ],
      "metadata": {
        "id": "TFscA7tZ0LcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Metric for Success\n",
        "The f1_score and accuracy have been used to measure the predictive power of the model before and after optimization."
      ],
      "metadata": {
        "id": "dS5RBcjtl-7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the Context\n",
        "This Dataset focuses on classifying Email as Spam or Non-Spam by frequency of word or character. The dataset was developed at Hewlett-Packard Labs and was donated by George Forman on July 1999 (Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt, 1999). The dataset contains 4601 instances and 58 variables. It contains two fields “Spam” and “Not Spam” for prediction. It is multivariate, real dataset mainly used for classification of attributes."
      ],
      "metadata": {
        "id": "lBx5GgVne2iB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recording the Experimental Design\n",
        "\n",
        "1. Read and explore the given dataset.\n",
        "\n",
        "2. Define the appropriateness of the available data to answer the given question. \n",
        "\n",
        "3. Find and deal with outliers, anomalies, and missing data within the dataset.\n",
        " \n",
        "4. Perform univariate, bivariate and multivariate analysis recording your observations.  \n",
        "\n",
        "5. Randomly partition the dataset into two parts i.e 80 - 20  sets.\n",
        "\n",
        "6. For the given dataset, because we don't have the label for the test set, we will use the train set to create train and test data (i.e. splitting further), then perform Naive Bayes classification.\n",
        "\n",
        "7. Compute the accuracy (percentage of correct classification).\n",
        "\n",
        "8. Report the confusion matrix of each classifier.\n",
        "\n",
        "9. Repeat step 2 to step 4 twice, each time splitting the datasets differently i.e. 70-30, 60-40, then note the outcomes of your modeling.\n",
        "\n",
        "10. Suggest and apply at least one of the optimization techniques that you learned earlier this week.\n",
        "\n",
        "11. Provide further recommendations."
      ],
      "metadata": {
        "id": "2DFEM2E9lfOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Relevance\n",
        "\n",
        "Dataset Source: [link text](https://archive.ics.uci.edu/ml/datasets/Spambase)\n",
        "\n",
        " **Rows**: 4,601 # **Columns**: 58\n",
        "\n",
        "**Target**: class\n",
        "\n",
        "**Features**\n",
        "Numeric: word_freq_make, word_freq_address, word_freq_all, word_freq_3d, word_freq_our, word_freq_over, word_freq_remove, word_freq_internet, word_freq_order, word_freq_mail, word_freq_receive, word_freq_will, word_freq_people, word_freq_report, word_freq_addresses, word_freq_free, word_freq_business, word_freq_email, word_freq_you, word_freq_credit, ...\n",
        "\n",
        "**Attributes Information:**\n",
        "\n",
        "Out of 58 variables, 48 attributes are continuous, real and determines the frequency of words like “data”, “telnet”, “technology”, “1999” and many more, 6 attributes are continuous and real and characters like “;”, “(”, “[“ and so on, 1 is continuous and real attribute named “capital_run_length_longest” which determines length of longest uninterrupted sequence of capital letters, 1 continuous and integer attribute named capital_run_length_total which determines sum of length of uninterrupted sequences of capital letters and last attribute is Class which determines whether it is spam or not by 0 and 1."
      ],
      "metadata": {
        "id": "YRQeJ3hnfRKX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries"
      ],
      "metadata": {
        "id": "-fCD2btO27R-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mWkGGmknHvRu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "ZWEm-ye5tW-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required modules\n",
        "from zipfile import ZipFile\n",
        "  \n",
        "# specifying the zip file name\n",
        "file_name = \"spambase.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    # printing all the contents of the zip file\n",
        "    zip.printdir()\n",
        "  \n",
        "    # extracting all the files\n",
        "    print('Extracting all the files now...')\n",
        "    zip.extractall()\n",
        "    print('Done!')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI91rAhFJu4B",
        "outputId": "dadf502a-3dd1-4612-c9c6-3deb84d614fd"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name                                             Modified             Size\n",
            "spambase.DOCUMENTATION                         1999-08-17 11:48:54         6429\n",
            "spambase.data                                  1999-08-16 22:24:14       702942\n",
            "spambase.names                                 1999-08-17 11:42:30         3566\n",
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('spambase.data', header=None)\n",
        "data.rename(columns={57:'is_spam'}, inplace=True)"
      ],
      "metadata": {
        "id": "kdnyripfTlWl"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data exploration :"
      ],
      "metadata": {
        "id": "fq3_NNDcwbKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "PIFE93fVTsgL",
        "outputId": "acec3d70-0319-439a-eaa0-64b14513f469"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>is_spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1     2    3     4     5  ...     52     53     54   55    56  is_spam\n",
              "0  0.00  0.64  0.64  0.0  0.32  0.00  ...  0.000  0.000  3.756   61   278        1\n",
              "1  0.21  0.28  0.50  0.0  0.14  0.28  ...  0.180  0.048  5.114  101  1028        1\n",
              "2  0.06  0.00  0.71  0.0  1.23  0.19  ...  0.184  0.010  9.821  485  2259        1\n",
              "3  0.00  0.00  0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191        1\n",
              "4  0.00  0.00  0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191        1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "9FOSDRp6wfVA",
        "outputId": "d39047da-cd54-4b04-de14-2bcfc983ac91"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>is_spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104553</td>\n",
              "      <td>0.213015</td>\n",
              "      <td>0.280656</td>\n",
              "      <td>0.065425</td>\n",
              "      <td>0.312223</td>\n",
              "      <td>0.095901</td>\n",
              "      <td>0.114208</td>\n",
              "      <td>0.105295</td>\n",
              "      <td>0.090067</td>\n",
              "      <td>0.239413</td>\n",
              "      <td>0.059824</td>\n",
              "      <td>0.541702</td>\n",
              "      <td>0.093930</td>\n",
              "      <td>0.058626</td>\n",
              "      <td>0.049205</td>\n",
              "      <td>0.248848</td>\n",
              "      <td>0.142586</td>\n",
              "      <td>0.184745</td>\n",
              "      <td>1.662100</td>\n",
              "      <td>0.085577</td>\n",
              "      <td>0.809761</td>\n",
              "      <td>0.121202</td>\n",
              "      <td>0.101645</td>\n",
              "      <td>0.094269</td>\n",
              "      <td>0.549504</td>\n",
              "      <td>0.265384</td>\n",
              "      <td>0.767305</td>\n",
              "      <td>0.124845</td>\n",
              "      <td>0.098915</td>\n",
              "      <td>0.102852</td>\n",
              "      <td>0.064753</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.097229</td>\n",
              "      <td>0.047835</td>\n",
              "      <td>0.105412</td>\n",
              "      <td>0.097477</td>\n",
              "      <td>0.136953</td>\n",
              "      <td>0.013201</td>\n",
              "      <td>0.078629</td>\n",
              "      <td>0.064834</td>\n",
              "      <td>0.043667</td>\n",
              "      <td>0.132339</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.079196</td>\n",
              "      <td>0.301224</td>\n",
              "      <td>0.179824</td>\n",
              "      <td>0.005444</td>\n",
              "      <td>0.031869</td>\n",
              "      <td>0.038575</td>\n",
              "      <td>0.139030</td>\n",
              "      <td>0.016976</td>\n",
              "      <td>0.269071</td>\n",
              "      <td>0.075811</td>\n",
              "      <td>0.044238</td>\n",
              "      <td>5.191515</td>\n",
              "      <td>52.172789</td>\n",
              "      <td>283.289285</td>\n",
              "      <td>0.394045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.305358</td>\n",
              "      <td>1.290575</td>\n",
              "      <td>0.504143</td>\n",
              "      <td>1.395151</td>\n",
              "      <td>0.672513</td>\n",
              "      <td>0.273824</td>\n",
              "      <td>0.391441</td>\n",
              "      <td>0.401071</td>\n",
              "      <td>0.278616</td>\n",
              "      <td>0.644755</td>\n",
              "      <td>0.201545</td>\n",
              "      <td>0.861698</td>\n",
              "      <td>0.301036</td>\n",
              "      <td>0.335184</td>\n",
              "      <td>0.258843</td>\n",
              "      <td>0.825792</td>\n",
              "      <td>0.444055</td>\n",
              "      <td>0.531122</td>\n",
              "      <td>1.775481</td>\n",
              "      <td>0.509767</td>\n",
              "      <td>1.200810</td>\n",
              "      <td>1.025756</td>\n",
              "      <td>0.350286</td>\n",
              "      <td>0.442636</td>\n",
              "      <td>1.671349</td>\n",
              "      <td>0.886955</td>\n",
              "      <td>3.367292</td>\n",
              "      <td>0.538576</td>\n",
              "      <td>0.593327</td>\n",
              "      <td>0.456682</td>\n",
              "      <td>0.403393</td>\n",
              "      <td>0.328559</td>\n",
              "      <td>0.555907</td>\n",
              "      <td>0.329445</td>\n",
              "      <td>0.532260</td>\n",
              "      <td>0.402623</td>\n",
              "      <td>0.423451</td>\n",
              "      <td>0.220651</td>\n",
              "      <td>0.434672</td>\n",
              "      <td>0.349916</td>\n",
              "      <td>0.361205</td>\n",
              "      <td>0.766819</td>\n",
              "      <td>0.223812</td>\n",
              "      <td>0.621976</td>\n",
              "      <td>1.011687</td>\n",
              "      <td>0.911119</td>\n",
              "      <td>0.076274</td>\n",
              "      <td>0.285735</td>\n",
              "      <td>0.243471</td>\n",
              "      <td>0.270355</td>\n",
              "      <td>0.109394</td>\n",
              "      <td>0.815672</td>\n",
              "      <td>0.245882</td>\n",
              "      <td>0.429342</td>\n",
              "      <td>31.729449</td>\n",
              "      <td>194.891310</td>\n",
              "      <td>606.347851</td>\n",
              "      <td>0.488698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.310000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.276000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.640000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.315000</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.706000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>266.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>5.550000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>5.450000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>20.830000</td>\n",
              "      <td>16.660000</td>\n",
              "      <td>33.330000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>6.890000</td>\n",
              "      <td>8.330000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>3.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>22.050000</td>\n",
              "      <td>2.170000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0            1  ...            56      is_spam\n",
              "count  4601.000000  4601.000000  ...   4601.000000  4601.000000\n",
              "mean      0.104553     0.213015  ...    283.289285     0.394045\n",
              "std       0.305358     1.290575  ...    606.347851     0.488698\n",
              "min       0.000000     0.000000  ...      1.000000     0.000000\n",
              "25%       0.000000     0.000000  ...     35.000000     0.000000\n",
              "50%       0.000000     0.000000  ...     95.000000     0.000000\n",
              "75%       0.000000     0.000000  ...    266.000000     1.000000\n",
              "max       4.540000    14.280000  ...  15841.000000     1.000000\n",
              "\n",
              "[8 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsyRuPs5danS",
        "outputId": "ffa845d2-8241-4351-bba4-00d4202e8c18"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4601 entries, 0 to 4600\n",
            "Data columns (total 58 columns):\n",
            " #   Column   Non-Null Count  Dtype  \n",
            "---  ------   --------------  -----  \n",
            " 0   0        4601 non-null   float64\n",
            " 1   1        4601 non-null   float64\n",
            " 2   2        4601 non-null   float64\n",
            " 3   3        4601 non-null   float64\n",
            " 4   4        4601 non-null   float64\n",
            " 5   5        4601 non-null   float64\n",
            " 6   6        4601 non-null   float64\n",
            " 7   7        4601 non-null   float64\n",
            " 8   8        4601 non-null   float64\n",
            " 9   9        4601 non-null   float64\n",
            " 10  10       4601 non-null   float64\n",
            " 11  11       4601 non-null   float64\n",
            " 12  12       4601 non-null   float64\n",
            " 13  13       4601 non-null   float64\n",
            " 14  14       4601 non-null   float64\n",
            " 15  15       4601 non-null   float64\n",
            " 16  16       4601 non-null   float64\n",
            " 17  17       4601 non-null   float64\n",
            " 18  18       4601 non-null   float64\n",
            " 19  19       4601 non-null   float64\n",
            " 20  20       4601 non-null   float64\n",
            " 21  21       4601 non-null   float64\n",
            " 22  22       4601 non-null   float64\n",
            " 23  23       4601 non-null   float64\n",
            " 24  24       4601 non-null   float64\n",
            " 25  25       4601 non-null   float64\n",
            " 26  26       4601 non-null   float64\n",
            " 27  27       4601 non-null   float64\n",
            " 28  28       4601 non-null   float64\n",
            " 29  29       4601 non-null   float64\n",
            " 30  30       4601 non-null   float64\n",
            " 31  31       4601 non-null   float64\n",
            " 32  32       4601 non-null   float64\n",
            " 33  33       4601 non-null   float64\n",
            " 34  34       4601 non-null   float64\n",
            " 35  35       4601 non-null   float64\n",
            " 36  36       4601 non-null   float64\n",
            " 37  37       4601 non-null   float64\n",
            " 38  38       4601 non-null   float64\n",
            " 39  39       4601 non-null   float64\n",
            " 40  40       4601 non-null   float64\n",
            " 41  41       4601 non-null   float64\n",
            " 42  42       4601 non-null   float64\n",
            " 43  43       4601 non-null   float64\n",
            " 44  44       4601 non-null   float64\n",
            " 45  45       4601 non-null   float64\n",
            " 46  46       4601 non-null   float64\n",
            " 47  47       4601 non-null   float64\n",
            " 48  48       4601 non-null   float64\n",
            " 49  49       4601 non-null   float64\n",
            " 50  50       4601 non-null   float64\n",
            " 51  51       4601 non-null   float64\n",
            " 52  52       4601 non-null   float64\n",
            " 53  53       4601 non-null   float64\n",
            " 54  54       4601 non-null   float64\n",
            " 55  55       4601 non-null   int64  \n",
            " 56  56       4601 non-null   int64  \n",
            " 57  is_spam  4601 non-null   int64  \n",
            "dtypes: float64(55), int64(3)\n",
            "memory usage: 2.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the data, except the class, is numerical\n",
        "\n",
        "39% of the emails in the dataset are classified as spams\n",
        "\n",
        "All the frequency data is between 0 and 100, all of the class data is 0 or 1 : so their is no outlier"
      ],
      "metadata": {
        "id": "g4CIumvmw1GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().any().any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPXkh3iAw_JX",
        "outputId": "24c92c06-4c10-41ad-c5b4-7612767375c2"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No missing data"
      ],
      "metadata": {
        "id": "2VHedk83xNK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving the Model Performance "
      ],
      "metadata": {
        "id": "Yfd85ds6mdOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing Data"
      ],
      "metadata": {
        "id": "qRHpj8hb3SzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's normalize our data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler() # define the scaler\n",
        "data = pd.DataFrame(sc.fit_transform(data)) # fit & transform the data\n",
        "# Display the Normalized data\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDWPZ8Tp3He2",
        "outputId": "55aac5d0-773f-469c-8065-16d81109b3e3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0         1         2    3   ...        54        55        56   57\n",
            "0  0.000000  0.044818  0.125490  0.0  ...  0.002502  0.006007  0.017487  1.0\n",
            "1  0.046256  0.019608  0.098039  0.0  ...  0.003735  0.010012  0.064836  1.0\n",
            "2  0.013216  0.000000  0.139216  0.0  ...  0.008008  0.048458  0.142551  1.0\n",
            "3  0.000000  0.000000  0.000000  0.0  ...  0.002303  0.003905  0.011995  1.0\n",
            "4  0.000000  0.000000  0.000000  0.0  ...  0.002303  0.003905  0.011995  1.0\n",
            "\n",
            "[5 rows x 58 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1679: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:1679: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Identify Highly Correlated Features"
      ],
      "metadata": {
        "id": "NZ6qu16n4Gyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create correlation matrix\n",
        "corr_matrix = data.corr().abs()\n",
        "\n",
        "# Select upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "\n",
        "# Find index of feature columns with correlation greater than 0.95\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]"
      ],
      "metadata": {
        "id": "gKSsCSea4BV7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Drop Marked Features"
      ],
      "metadata": {
        "id": "m2p37d5B4o9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop features \n",
        "data.drop(data[to_drop], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "UKbpPubM4poh",
        "outputId": "694fb381-d4f1-4467-ad4b-7de85c913d3f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044818</td>\n",
              "      <td>0.125490</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.066184</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.141914</td>\n",
              "      <td>0.102933</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086409</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023955</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002502</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>0.017487</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.046256</td>\n",
              "      <td>0.019608</td>\n",
              "      <td>0.098039</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.014</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.028886</td>\n",
              "      <td>0.006301</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.051705</td>\n",
              "      <td>0.080460</td>\n",
              "      <td>0.081696</td>\n",
              "      <td>0.117117</td>\n",
              "      <td>0.021</td>\n",
              "      <td>0.031746</td>\n",
              "      <td>0.0070</td>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.030803</td>\n",
              "      <td>0.185067</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.143114</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.078899</td>\n",
              "      <td>0.0344</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013536</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011454</td>\n",
              "      <td>0.029985</td>\n",
              "      <td>0.002421</td>\n",
              "      <td>0.003735</td>\n",
              "      <td>0.010012</td>\n",
              "      <td>0.064836</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.013216</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.139216</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.123</td>\n",
              "      <td>0.032313</td>\n",
              "      <td>0.026135</td>\n",
              "      <td>0.010801</td>\n",
              "      <td>0.121673</td>\n",
              "      <td>0.013751</td>\n",
              "      <td>0.145594</td>\n",
              "      <td>0.046536</td>\n",
              "      <td>0.021622</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.396825</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.113311</td>\n",
              "      <td>0.072533</td>\n",
              "      <td>0.017602</td>\n",
              "      <td>0.045905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.212844</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.012605</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.033613</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.002801</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002281</td>\n",
              "      <td>0.014664</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008498</td>\n",
              "      <td>0.030651</td>\n",
              "      <td>0.000504</td>\n",
              "      <td>0.008008</td>\n",
              "      <td>0.048458</td>\n",
              "      <td>0.142551</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042641</td>\n",
              "      <td>0.056706</td>\n",
              "      <td>0.058935</td>\n",
              "      <td>0.034653</td>\n",
              "      <td>0.118774</td>\n",
              "      <td>0.032058</td>\n",
              "      <td>0.055856</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027903</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014048</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004218</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002303</td>\n",
              "      <td>0.003905</td>\n",
              "      <td>0.011995</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.063</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042641</td>\n",
              "      <td>0.056706</td>\n",
              "      <td>0.058935</td>\n",
              "      <td>0.034653</td>\n",
              "      <td>0.118774</td>\n",
              "      <td>0.032058</td>\n",
              "      <td>0.055856</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0155</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.169600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027903</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013843</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004157</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002303</td>\n",
              "      <td>0.003905</td>\n",
              "      <td>0.011995</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4596</th>\n",
              "      <td>0.068282</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.121569</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.052721</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.194416</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033067</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0155</td>\n",
              "      <td>0.014472</td>\n",
              "      <td>0.014059</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.023790</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.005492</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4597</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.180018</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.090703</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010869</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000504</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000821</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4598</th>\n",
              "      <td>0.066079</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.186143</td>\n",
              "      <td>0.054054</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.099010</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054422</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023261</td>\n",
              "      <td>0.073626</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>0.007386</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599</th>\n",
              "      <td>0.211454</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.033092</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.102933</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.028803</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014512</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005845</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.004861</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4600</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.127451</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.117117</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.245333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.058506</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.091970</td>\n",
              "      <td>0.029478</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003849</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.002462</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4601 rows × 57 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2    3   ...        54        55        56   57\n",
              "0     0.000000  0.044818  0.125490  0.0  ...  0.002502  0.006007  0.017487  1.0\n",
              "1     0.046256  0.019608  0.098039  0.0  ...  0.003735  0.010012  0.064836  1.0\n",
              "2     0.013216  0.000000  0.139216  0.0  ...  0.008008  0.048458  0.142551  1.0\n",
              "3     0.000000  0.000000  0.000000  0.0  ...  0.002303  0.003905  0.011995  1.0\n",
              "4     0.000000  0.000000  0.000000  0.0  ...  0.002303  0.003905  0.011995  1.0\n",
              "...        ...       ...       ...  ...  ...       ...       ...       ...  ...\n",
              "4596  0.068282  0.000000  0.121569  0.0  ...  0.000129  0.000200  0.005492  0.0\n",
              "4597  0.000000  0.000000  0.000000  0.0  ...  0.000504  0.000300  0.000821  0.0\n",
              "4598  0.066079  0.000000  0.058824  0.0  ...  0.000367  0.000501  0.007386  0.0\n",
              "4599  0.211454  0.000000  0.000000  0.0  ...  0.000133  0.000400  0.004861  0.0\n",
              "4600  0.000000  0.000000  0.127451  0.0  ...  0.000227  0.000400  0.002462  0.0\n",
              "\n",
              "[4601 rows x 57 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no highly correlated features"
      ],
      "metadata": {
        "id": "I0lUJXx-6fR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking and Handling Imbalanced Classes"
      ],
      "metadata": {
        "id": "wuMQmEkDMTNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_count = data.is_spam.value_counts()\n",
        "print('Class 0:', target_count[0])\n",
        "print('Class 1:', target_count[1])\n",
        "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZuidhbvMOk9",
        "outputId": "072c5ce5-27e1-4de8-c344-00046990b0a4"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0: 2788\n",
            "Class 1: 1813\n",
            "Proportion: 1.54 : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, **proportion of classes**,\n",
        "\n",
        "You can see, it is totally unbalanced data with **1.54 : 1** proportion.\n",
        "\n",
        "**Let see with the graph**."
      ],
      "metadata": {
        "id": "fqKffqRKMinE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_count.plot(kind='bar', title='Count (is_spam)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "hSkL4Sa1MzrE",
        "outputId": "aaa72540-bd8e-4e96-8dfa-07a537788215"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fad15357210>"
            ]
          },
          "metadata": {},
          "execution_count": 108
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority of the data belong to ham class i.e 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "o0WQNfUWM11z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random under sampling\n",
        "\n",
        "How its work you had 1813 is spam messages and 2788 is ham messages.\n",
        "\n",
        "So its unbalance dataset , for do balance we are applying **under-sampling**."
      ],
      "metadata": {
        "id": "hamVnK16J4Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_class_0, count_class_1 = data.is_spam.value_counts()\n",
        "spam = data[data['is_spam'] == 1]\n",
        "ham = data[data['is_spam'] == 0]"
      ],
      "metadata": {
        "id": "aTjZZJh1Uh8_"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_class_0_under = ham.sample(count_class_1)\n",
        "data_test_under = pd.concat([data_class_0_under, spam], axis=0)\n",
        "print('Random under-sampling:')\n",
        "print(data_test_under.is_spam.value_counts())\n",
        "data_test_under.is_spam.value_counts().plot(kind='bar', title='Count (is_spam)');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "VZfBzcYJIrQN",
        "outputId": "e320d311-e4dd-4bba-8e6e-6dab63652996"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random under-sampling:\n",
            "1    1813\n",
            "0    1813\n",
            "Name: is_spam, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right here, 1813 samples random picked up from ham which represent with 0 .\n",
        "\n",
        "Total ham data -2788( randomly picked 1813 data point from ham class by under-sampling)\n",
        "\n",
        "Total spam data — 1813\n",
        "\n",
        "Total data is 1813+1813 with equal ratio.\n",
        "\n",
        "So, data is balanced using under-sampling."
      ],
      "metadata": {
        "id": "-fpzPGnhKeXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling"
      ],
      "metadata": {
        "id": "T2Cj1O-knOYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "ZctOGQorGdkI",
        "outputId": "164ea398-b811-4796-c49a-c91c12a5088c"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>is_spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      0     1     2    3     4     5  ...     52     53     54   55    56  is_spam\n",
              "0  0.00  0.64  0.64  0.0  0.32  0.00  ...  0.000  0.000  3.756   61   278        1\n",
              "1  0.21  0.28  0.50  0.0  0.14  0.28  ...  0.180  0.048  5.114  101  1028        1\n",
              "2  0.06  0.00  0.71  0.0  1.23  0.19  ...  0.184  0.010  9.821  485  2259        1\n",
              "3  0.00  0.00  0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191        1\n",
              "4  0.00  0.00  0.00  0.0  0.63  0.00  ...  0.000  0.000  3.537   40   191        1\n",
              "\n",
              "[5 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting and Training Dataset(80-20 sets)"
      ],
      "metadata": {
        "id": "vQ-GLniZrpx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split our dataset into four subsets, X_train, X_test, y_train, and y_test."
      ],
      "metadata": {
        "id": "KAj5hTRyaprF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting our dataset into test and train dataset \n",
        "spam_train, spam_test = train_test_split(spam, train_size=0.8)\n",
        "ham_train, ham_test = train_test_split(ham, train_size=0.8)"
      ],
      "metadata": {
        "id": "24Prkf45VBS7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = ham_train.append(spam_train)\n",
        "y_train = X_train.pop('is_spam')"
      ],
      "metadata": {
        "id": "CHtcPPYsPUYN"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = ham_test.append(spam_test)\n",
        "y_test = X_test.pop('is_spam')"
      ],
      "metadata": {
        "id": "VL02NZEWVbPe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## printing Output"
      ],
      "metadata": {
        "id": "KOmTv2bUnvCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U31Qo1H3ZbQV",
        "outputId": "6ecd2498-ea07-458d-91e5-212cb0358f2e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0     1     2    3     4     5   ...     51     52     53     54  55   56\n",
            "3684  0.00  0.00  0.00  0.0  0.00  0.00  ...  0.000  0.000  0.000  1.000   1    3\n",
            "3180  0.00  0.00  0.00  0.0  1.47  0.00  ...  0.000  0.000  0.000  3.037  15   82\n",
            "2445  0.12  0.00  0.25  0.0  0.00  0.00  ...  0.000  0.036  0.000  3.167  32  491\n",
            "2593  0.00  0.00  0.18  0.0  0.00  0.09  ...  0.000  0.000  0.000  2.022  19  451\n",
            "3802  0.00  0.00  0.50  0.0  0.50  0.00  ...  0.055  0.000  0.000  4.275  45  248\n",
            "...    ...   ...   ...  ...   ...   ...  ...    ...    ...    ...    ...  ..  ...\n",
            "635   0.00  0.64  0.64  0.0  0.32  0.00  ...  1.065  0.000  0.000  3.932  61  291\n",
            "1598  0.09  0.09  1.14  0.0  0.38  0.00  ...  0.591  0.000  0.000  3.280  31  771\n",
            "453   0.00  0.00  0.00  0.0  0.00  0.00  ...  0.000  0.000  0.000  1.000   1    2\n",
            "1445  0.20  0.10  0.70  0.0  1.10  0.20  ...  0.530  0.406  0.123  9.781  84  851\n",
            "1344  0.09  0.00  0.27  0.0  0.36  0.09  ...  0.257  0.032  0.032  3.689  69  535\n",
            "\n",
            "[3680 rows x 57 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZDh1ESpZjXc",
        "outputId": "f0176363-ed9a-4ea1-c3b0-7fb74b35771c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        0     1     2    3     4     5   ...     51     52     53     54  55   56\n",
            "4249  0.00  0.35  0.35  0.0  0.00  0.35  ...  0.000  0.000  0.000  1.787  11  118\n",
            "3037  0.00  0.00  0.00  0.0  3.84  0.00  ...  0.000  0.000  0.000  1.000   1    7\n",
            "1881  0.00  0.00  0.00  0.0  0.00  0.00  ...  0.000  0.000  0.000  1.600   9   24\n",
            "2108  0.00  0.00  0.00  0.0  0.00  0.00  ...  0.000  0.000  0.000  1.583   8   38\n",
            "1981  0.81  0.00  0.81  0.0  0.81  0.00  ...  0.370  0.000  0.000  5.375  69  129\n",
            "...    ...   ...   ...  ...   ...   ...  ...    ...    ...    ...    ...  ..  ...\n",
            "1521  0.00  0.00  0.00  0.0  0.00  0.00  ...  0.000  0.000  0.000  1.545   4   17\n",
            "788   0.09  0.00  0.27  0.0  0.36  0.09  ...  0.252  0.031  0.031  3.816  69  542\n",
            "1776  0.00  0.21  0.43  0.0  0.65  0.00  ...  0.960  0.128  0.128  8.080  70  501\n",
            "589   0.00  0.00  0.00  0.0  0.00  0.00  ...  0.000  0.000  0.000  2.769  15   36\n",
            "481   0.00  0.00  0.00  0.0  0.00  0.00  ...  4.347  0.000  0.000  1.000   1    2\n",
            "\n",
            "[921 rows x 57 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fb34rejZs98",
        "outputId": "3a9a58ec-feb4-49f5-c148-671df2414b74"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3684    0\n",
            "3180    0\n",
            "2445    0\n",
            "2593    0\n",
            "3802    0\n",
            "       ..\n",
            "635     1\n",
            "1598    1\n",
            "453     1\n",
            "1445    1\n",
            "1344    1\n",
            "Name: is_spam, Length: 3680, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAGEF4anZzqo",
        "outputId": "e0f030a4-c97d-4540-b588-e323270f160b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4249    0\n",
            "3037    0\n",
            "1881    0\n",
            "2108    0\n",
            "1981    0\n",
            "       ..\n",
            "1521    1\n",
            "788     1\n",
            "1776    1\n",
            "589     1\n",
            "481     1\n",
            "Name: is_spam, Length: 921, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting our model"
      ],
      "metadata": {
        "id": "nymHHqpZsXAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting our model \n",
        "# Then, all that we have to do is initialize the Naive Bayes Classifier and fit the data. \n",
        "# For text classification problems, the Multinomial Naive Bayes Classifier is well-suited"
      ],
      "metadata": {
        "id": "3a7XZ84EsT18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb5dpk-XVtEh",
        "outputId": "be8ef6af-853d-41fc-ddeb-859bafcef0f2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes = MultinomialNB()\n",
        "spam_bayes.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aoe0U6E5V2pX",
        "outputId": "647fff01-6102-4cc8-b416-9dedd8ff789b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model"
      ],
      "metadata": {
        "id": "9lAnYc9Gsvaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Model\n",
        "# Once we have put together our classifier, we can evaluate its performance in the testing set"
      ],
      "metadata": {
        "id": "TQGBFElnss9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtV44thBWDwF",
        "outputId": "ac4c6132-17f5-488a-cab4-fe88b140e407"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8078175895765473"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing our predicted score with our actual score to determine our model prediction power"
      ],
      "metadata": {
        "id": "glPLq_i6zrLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes.score(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz3jsBaezbk2",
        "outputId": "e8ce88f4-314b-4b7f-898e-9b0c34aa9b86"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7891304347826087"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Confusion Matrix and Classification Reports"
      ],
      "metadata": {
        "id": "9Plyd-UDzRUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting y_pred by predicting over X_text and flattening it\n",
        "y_pred = spam_bayes.predict(X_test)\n",
        "y_pred = y_pred.flatten() # require to be in one-dimensional array , for easy manipulation"
      ],
      "metadata": {
        "id": "Wy8zVJAdvwEM"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing confusion maxtrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "# creating confusion matrix \n",
        "\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDkjXVl_vcH1",
        "outputId": "2002f16a-2eed-4f64-c695-ba4911d2c493"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[477,  81],\n",
              "       [ 96, 267]])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting as a graph - importing seaborn\n",
        "import seaborn as sns\n",
        "# creating a graph out of confusion matrix\n",
        "sns.heatmap(cm, annot = True, fmt = 'd')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "oMuLXtYNw9yX",
        "outputId": "c858b8b6-fff3-40f9-b3dd-6a1017a57d42"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Actual')"
            ]
          },
          "metadata": {},
          "execution_count": 115
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen for X-axis – We have Predicted Values, For Y-axis – We have Actual Values. Also at diagonal, we have model correct predictions.\n",
        "\n",
        "Through the graph we can see that out of a total, 477 times the mail was ham (0) and the model predicted it right and for 267 times it was spam and model predicted spam(1), so overall we have created a good model, however one can experiment with the parameters, layers and network architecture to increase it\n",
        "\n",
        "**Classification report is also plotted similarly:**"
      ],
      "metadata": {
        "id": "3UJbsLscxzKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing classification report\n",
        "print(classification_report(y_test , y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AagSuHGyHSj",
        "outputId": "6515db27-2383-4764-ed36-e7c9750c7e94"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84       558\n",
            "           1       0.77      0.74      0.75       363\n",
            "\n",
            "    accuracy                           0.81       921\n",
            "   macro avg       0.80      0.80      0.80       921\n",
            "weighted avg       0.81      0.81      0.81       921\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here also it is evident that the model is a good one as recall and accuracy us good"
      ],
      "metadata": {
        "id": "pxu805poypHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting and Training Dataset(70-30 sets)"
      ],
      "metadata": {
        "id": "mAOfevuIb2ET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's split our dataset into four subsets, X_train, X_test, y_train, and y_test."
      ],
      "metadata": {
        "id": "QfWIwfAxbwqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting our dataset into train and test datas\n",
        "spam_train, spam_test = train_test_split(spam, train_size=0.7)\n",
        "ham_train, ham_test = train_test_split(ham, train_size=0.7)"
      ],
      "metadata": {
        "id": "oTf7EF29bbib"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = ham_train.append(spam_train)\n",
        "y_train = X_train.pop('is_spam')"
      ],
      "metadata": {
        "id": "v5gXQqsgbUnA"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = ham_test.append(spam_test)\n",
        "y_test = X_test.pop('is_spam')"
      ],
      "metadata": {
        "id": "cXbs3esJbMUy"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting model"
      ],
      "metadata": {
        "id": "55_SJYBaooN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting our model \n",
        "# Then, all that we have to do is initialize the Naive Bayes Classifier and fit the data. \n",
        "# For text classification problems, the Multinomial Naive Bayes Classifier is well-suited\n",
        "#\n",
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG_c3f-FbIhr",
        "outputId": "c24dd133-dee1-48f2-b1d1-ef49cdbff006"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes = MultinomialNB()\n",
        "spam_bayes.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNj6t3iibFa6",
        "outputId": "73ad7523-944b-4969-d824-3f053953dd73"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model"
      ],
      "metadata": {
        "id": "UgNwavsCo7dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Model\n",
        "# Once we have put together our classifier, we can evaluate its performance in the testing set"
      ],
      "metadata": {
        "id": "fCi49QnDo-M1"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i-9jMhUbCWq",
        "outputId": "1f43eb84-97ff-4dbe-a1a6-fb4168ff8ee8"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8102824040550326"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing our predicted score with our actual score to determine our model prediction power"
      ],
      "metadata": {
        "id": "ThdQ47P-qXKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes.score(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76-ILsksWkXR",
        "outputId": "be3ebd92-37dd-4857-9c68-671dbea19248"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8031055900621118"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Confusion Matrix and Classification Reports"
      ],
      "metadata": {
        "id": "LyJfyz1mzMP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting y_pred by predicting over X_text and flattening it\n",
        "y_pred = spam_bayes.predict(X_test)\n",
        "y_pred = y_pred.flatten() # require to be in one-dimensional array , for easy manipulation"
      ],
      "metadata": {
        "id": "oOjb7X9_wV5N"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing confusion maxtrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "# creating confusion matrix \n",
        "\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5fX_vVCwal6",
        "outputId": "a67aee24-0f56-4fdc-db81-804070b40d30"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[709, 128],\n",
              "       [134, 410]])"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a graph out of confusion matrix\n",
        "sns.heatmap(cm, annot = True, fmt = 'd')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "_ZgfALPoxBnk",
        "outputId": "d392331b-8cf4-45dc-d3e4-dab05a9e9b20"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Actual')"
            ]
          },
          "metadata": {},
          "execution_count": 126
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEHCAYAAACOWawdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAZuUlEQVR4nO3deZhV1Znv8e8PEFQcAFHmKCpq7HREUETTMSpOoBHNNVxNWonBVDrB2NrxxukmaYdrTLdDa2sbUZKAM5EYaYMjapRWEYJGo8bHEkVAZFBExAGq6r1/nAUctYZTWKdOreL34VlP7b32tMoH3/Py7rX3UURgZmb56FDpAZiZWfM4cJuZZcaB28wsMw7cZmaZceA2M8uMA7eZWWY6VXoADVm7fJ7nKdpnbNH3q5UegrVBNWsW6fOeozkxZ7OeOzd4PUm7A3cUde0M/AyYnPp3Al4HxkTECkkCrgJGAR8A34mIuY1d3xm3mVkLioiXI2JwRAwGhlIIxncB5wAzImIQMCOtA4wEBqVWBVzX1DUcuM3MAOpqS2+lGwG8GhHzgdHApNQ/CTg2LY8GJkfBU0A3SX0aO2mbLZWYmbWq2ppynPUE4La03CsiFqflt4BeabkfsKDomIWpbzENcMZtZgZE1JXcJFVJmlPUqj59PkmdgWOA3332WhHARt/Hc8ZtZgZQV1fyrhExAZjQxG4jgbkRsSStL5HUJyIWp1LI0tS/CBhQdFz/1NcgZ9xmZgBRV3orzYlsKJMATAPGpuWxwN1F/SerYDiwsqikUi9n3GZm0Nybjo2S1BU4DPh+UfelwBRJ44D5wJjUP53CVMBqCjNQTmnq/A7cZmbQnEy66VNFrAa2+1Tf2xRmmXx63wDGN+f8DtxmZkCUZ1ZJWThwm5lBs25OVpoDt5kZtGippNwcuM3MoEVvTpabA7eZGTjjNjPLjm9OmpllxjcnzczyEuEat5lZXlzjNjPLjEslZmaZccZtZpaZ2rWVHkHJHLjNzMClEjOz7LhUYmaWGWfcZmaZceA2M8tL+OakmVlmXOM2M8uMSyVmZplxxm1mlhln3GZmmXHGbWaWmRp/kYKZWV6ccZuZZcY1bjOzzDjjNjPLjDNuM7PMOOM2M8tMRrNKOlR6AGZmbUJE6a0JkrpJulPS3yS9JGl/ST0kPSjplfSze9pXkq6WVC3pOUlDmjq/A7eZGRRq3KW2pl0F3BcRewB7AS8B5wAzImIQMCOtA4wEBqVWBVzX1MkduM3MoMUCt6RtgQOBiQARsSYi3gVGA5PSbpOAY9PyaGByFDwFdJPUp7FrOHCbmUHh5mSJTVKVpDlFraroTAOBZcBvJD0j6UZJXYFeEbE47fMW0Cst9wMWFB2/MPU1yDcnzcwAamtL3jUiJgATGtjcCRgC/CgiZkm6ig1lkXXHh6Smi+UNcMZtZgYtWeNeCCyMiFlp/U4KgXzJuhJI+rk0bV8EDCg6vn/qa5ADt5kZtFjgjoi3gAWSdk9dI4AXgWnA2NQ3Frg7LU8DTk6zS4YDK4tKKvVyqcTMDFr6AZwfAbdI6gzMA06hkChPkTQOmA+MSftOB0YB1cAHad9GOXCbmQFRt9El58+eK+JZYJ96No2oZ98Axjfn/A7cZmbgd5WYmWWnGbNKKs2B28wMnHFb6V6bv5CzfvaL9esL31zMaaeexDEjD+XHP/0Fb761hL69e3H5Reey7TZbs/K9Vfz0F1eyYNFiunTuzEXnncmgnXeq3C9gZXPDhMs5atShLF22nMF7F0qjv/zF/+Woow9jzZo1zJs3n3Gn/gsrV75Hp06dmHD9Zey995fo1KkTN998J7/8t2sq/BtkJqPA7emAFTZwx/5MnXQtUyddy5RfX83mm2/OiK8dwI03TWH4PoOZfsdEhu8zmIk3TwHghsl3sMegXbhr8nVc8tOzuPQ/flXh38DKZfLkKRx19Lc/0ffQjMfYa/AhDBl6GK+8Mo9zzj4NgOOPP5ouXTqz95BDGbbfkXzv1H9kxx37V2LY+WrBl0yVmwN3G/LUnGcZ0K8PfXv34pHHn2T0yEMBGD3yUB5+7EkAXn39DfYbshcAO+84gEWLl7D8nRUVG7OVz+MzZ/HOinc/0ffgQ49Rm2qxT82aS79+hVdaRARdu25Jx44d2WKLLVizdi3vvfd+q485ay37kqmyKlvglrSHpLPT6wqvTstfLNf12oN7Z/yJUYd+DYC3V7zL9j17ANBzu+68nf4H3n3XnXnoT/8DwPMvvsziJUtZsnR5ZQZsFXXKd07gvvsfAWDq1D+yevUHLHzjGV579WmuuOJXrPhU0Lcm1EXprcLKErglnQ3cDgh4OjUBt0k6p7FjN1Vr167l0ZmzOPyQr35mmyQkAXDqSd9k1fur+V9jx3PLndPYY9AudOzgfzhtas4953Rqamq49dbfAzBs38HU1tYyYMch7LrbcM488/sMHPiFCo8yM7W1pbcKK9fNyXHA30XE2uJOSVcALwCX1ndQesNWFcB/XX4xp558YpmG1/Y8/tQcvrjbLvTs0R2A7bp3Y9nyd9i+Zw+WLX+HHt22BWCrrl25+Px/AQr/PD7i+O/Qv1/vio3bWt/JJ43hqFGHctgRY9b3nXDCcdz/wKPU1NSwbNnbPPHEbIYO3YvXXnujgiPNS7SBEkipypWq1QF96+nvk7bVKyImRMQ+EbHPphS0AaY/+CijDjto/fpB/zCcu+99CIC7732Ig7+6PwDvrXqftWsLn4dT//s+hg7+e7bq2rXVx2uVccThB3HWWT/g2G98hw8//Gh9/4IFizj4oK8AsOWWW7DffkN4+eXqSg0zTxmVSsqVcZ8BzJD0ChveM/sFYFfgtDJdM1sffPgRT85+hp//5PT1faeeNIYf//QSfn/P/fTtvQOXX3QeAPPmL+D8iy9HwC4Dd+TCc8+o0Kit3G6+6Vq+duD+9OzZg9fnzeGCCy/j7J+cRpcuXbjv3tsBmDVrLuNPO4f/uu63TLzxSv7y7MNIYtKkO3j++Zcq/BtkJqMvC1aUaWqLpA7AMDa8EHwRMDsiSioQrV0+r/Ifa9bmbNH3s/cAzGrWLNLnPcfqC79dcszp+rNbPvf1Po+yPYATEXXAU+U6v5lZi6qp/E3HUvnJSTMzyKpU4sBtZgZt4qZjqRy4zczIazqgA7eZGTjjNjPLjgO3mVlm2sCj7KVy4DYzo2W/c7LcHLjNzMClEjOz7HhWiZlZZpxxm5llxoHbzCwvUetSiZlZXpxxm5nlxdMBzcxyk1Hg9rfMmplB4UsVS21NkPS6pOclPStpTurrIelBSa+kn91TvyRdLala0nOShjR1fgduMzMgaupKbiU6OCIGR8Q+af0cYEZEDAJmpHWAkcCg1KqA65o6sQO3mRm0aMbdgNHApLQ8CTi2qH9yFDwFdJPUp7ETOXCbmVG4OVlqK+V0wAOS/iypKvX1iojFafktoFda7seGL1UHWMiG7+qtl29OmplBszLpFIyriromRMSEovV/iIhFknYAHpT0t+LjIyIkbfTdUAduMzOaNx0wBekJjWxflH4ulXQXMAxYIqlPRCxOpZClafdFwICiw/unvga5VGJmBi1W45bUVdLW65aBw4G/AtOAsWm3scDdaXkacHKaXTIcWFlUUqmXM24zMyBqWuxUvYC7JEEhxt4aEfdJmg1MkTQOmA+MSftPB0YB1cAHwClNXcCB28wMiBZ6VUlEzAP2qqf/bWBEPf0BjG/ONRy4zczg80zza3UO3GZmtFzG3RocuM3McOA2M8tO1KrSQyiZA7eZGc64zcyyE3XOuM3MsuKM28wsMxHOuM3MsuKM28wsM3WeVWJmlhffnDQzy0y7CNyS/pPCtzjUKyJOL8uIzMwqIPL5kvdGM+45rTYKM7MKaxcZd0RMamibmVl7066mA0raHjgb2BPYfF1/RBxSxnGZmbWq2oxmlZTy1WW3AC8BA4ELgNeB2WUck5lZq4tQya3SSgnc20XERGBtRPwpIr4LONs2s3Yl6lRyq7RSpgOuTT8XSzoKeBPoUb4hmZm1vvYyq2SdiyVtC/wY+E9gG+DMso7KzKyVtYVMulRNBu6IuCctrgQOLu9wzMwqo7aulMpx21DKrJLfUM+DOKnWbWbWLrS3Usk9RcubA8dRqHObmbUbdW1gtkipSimVTC1el3QbMLNsIzIzq4C2MM2vVBvzkqlBwA4tPRAzs0pqV6USSav4ZI37LQpPUpZV134HlvsSlqHHthte6SFYO9XeSiVbt8ZAzMwqKadZJU2OVNKMUvrMzHIWzWiV1mDglrS5pB5AT0ndJfVIbSegX2sN0MysNdSFSm6lkNRR0jOS7knrAyXNklQt6Q5JnVN/l7Renbbv1NS5G8u4vw/8Gdgj/VzX7gauKWnkZmaZKMNLpv6Zwgv61vklcGVE7AqsAMal/nHAitR/ZdqvUQ0G7oi4KiIGAmdFxM4RMTC1vSLCgdvM2pW6ZrSmSOoPHAXcmNZF4eV8d6ZdJgHHpuXRaZ20fUTav0GlVOPrJHUrGlB3ST8s4Tgzs2wEKrmV4D+An7Ahzm8HvBsRNWl9IRtKzv2ABQBp+8q0f4NKCdzfi4h31/9yESuA75UycjOzXNSESm6SqiTNKWpV684j6WhgaUT8uVxjLeUBnI6SFFGYni6pI9C5XAMyM6uEEjPpwr4RE4AJDWz+CnCMpFEUXhOyDXAV0E1Sp5RV9wcWpf0XAQOAhZI6AdsCbzd2/VIy7vuAOySNkDQCuA24t4TjzMyy0VI17og4NyL6R8ROwAnAwxHxbeAR4Pi021gKEz0ApqV10vaH1yXKDSkl4z4bqAL+Ka0/B/Qu4Tgzs2w0J+PeSGcDt0u6GHgGmJj6JwI3SaoG3qEQ7BtVypOTdZJmAbsAY4CewNTGjzIzy0sps0WaKyIeBR5Ny/OAYfXs8xHwzeact8HALWk34MTUlgN3pIv4yxTMrN2pLX/G3WIay7j/BjwOHB0R1QCS/JVlZtYuZfTNZY3enPwGsBh4RNIN6cZkRr+amVnp6lDJrdIae3LyDxFxAoVH3h8BzgB2kHSdpMNba4BmZq2hXbxkap2IWB0Rt0bE1ynMPXyGVngft5lZa2rJR97LrVnfgJOemmxs4rmZWZbqGn89SJuyMV9dZmbW7tRWegDN4MBtZkZes0ocuM3MoE3MFimVA7eZGW1jtkipHLjNzHCpxMwsO21hml+pHLjNzIBaZ9xmZnlxxm1mlhkHbjOzzIRLJWZmeXHGbWaWGT/ybmaWGc/jNjPLjEslZmaZceA2M8uM31ViZpYZ17jNzDLjWSVmZpmpy6hY4sBtZoZvTpqZZSeffNuB28wMcMZtZpadGuWTc3eo9ADMzNqCaEZrjKTNJT0t6S+SXpB0QeofKGmWpGpJd0jqnPq7pPXqtH2npsbqwG1mRqFUUmprwsfAIRGxFzAYOFLScOCXwJURsSuwAhiX9h8HrEj9V6b9GuXAbWZGYTpgqa0xUfB+Wt0stQAOAe5M/ZOAY9Py6LRO2j5CUqOPAzlwm5nRvFKJpCpJc4paVfG5JHWU9CywFHgQeBV4NyJq0i4LgX5puR+wACBtXwls19hYfXPSzIzmzSqJiAnAhEa21wKDJXUD7gL2+JzD+wQHbjMzoLYMM7kj4l1JjwD7A90kdUpZdX9gUdptETAAWCipE7At8HZj53WpxMyMlrs5KWn7lGkjaQvgMOAl4BHg+LTbWODutDwtrZO2PxwRjX6KOOM2MwOi5TLuPsAkSR0pJMdTIuIeSS8Ct0u6GHgGmJj2nwjcJKkaeAc4oakLOHCbmdFyT05GxHPA3vX0zwOG1dP/EfDN5lzDpZI2YML1l7FwwbM8M/eh9X3/+vOz+POcB5n99P388Y+30KdPr08cM3ToXnyw+nW+cdxRrT1ca00dOvDlB/6dPSafC0DvU0ay9xPXcMDiqXTqsfUndh140XfZ+4lr2GvGFXT9+4GVGG3WWmo6YGtw4G4DJt/0O47++j9+ou/yK37F0H0OY99hRzB9+gzOP/+M9ds6dOjAJf/vPB586LHWHqq1sj7fO4oPX1m0fn3V7L/x4pgL+GjB0k/s1+2QIWy+cx+eOeA0Xv0/17HzpVWfPpU1oaWenGwNDtxtwMyZs1ix4t1P9K1a9f765a5bbkHxvYrx40/hrj9MZ9nS5a02Rmt9nfv0oPuIISy5dcO/xFb/9TU+XrjsM/v2OHJflv3uTwC8P/cVOm3Tlc126NZqY20PaoiSW6U5cLdhF17wE16tfpoTTzyOCy64DIC+fXsz+piRXH/95AqPzspt4IXfZf7FN0Fd04Gic+8efPzmhg/yjxe/Tec+jT7DYZ8SzfhTaa0euCWd0si29U8j1dWubs1htUk/+/m/scuuw7jttrv44Q8K/9kuv+xfOe/8S2hitpBlrvuhQ1m7fCWrn5tX6aFsMlrwXSVlV4lZJRcAv6lvQ/HTSJ279HdkSm67/S6m3T2ZCy+6nCFDv8zNN10LQM+ePTjyyEOoqa1h2rT7KzxKa0lbD9uD7ofvy5ARQ+jQZTM6br0lg645nVdOu7re/de89Q5d+vZkVVrv0mc71ixu9BkO+5S2kEmXqiyBW9JzDW0CejWwzYrsuutAqqtfA+DrXz+Cl19+FYDddz9g/T433nAF06fPcNBuh9645BbeuOQWALbZ/+/o+4NjGgzaACvun03v745k+R9mstWQQdSs+oC1S99tcH/7rLaQSZeqXBl3L+AICq8uLCbgiTJdM1s3Tb6GAw/cn549ezDv1dlceNHljDzyEHbbbWfq6oI33ljI+NPOrfQwrQ3oPW4U/X54LJ136MbgGVewYsZcXj3rOlbMmEu3EUMY8uS11H74MdVnXlvpoWanNqPyo8pRK5U0EfhNRMysZ9utEfGtps7hUonV59Ee+1V6CNYGHbB4aqOvQS3Ft3Y8ruSYc+v8uz739T6PsmTcETGukW1NBm0zs9a2yde4zcxy4xq3mVlm2sKj7KVy4DYzw6USM7Ps5DSrxIHbzAyXSszMsuObk2ZmmXGN28wsMy6VmJllJqc3bjpwm5kBtc64zczy4lKJmVlmXCoxM8uMM24zs8x4OqCZWWb8yLuZWWZcKjEzy4wDt5lZZnKaVdKh0gMwM2sL6oiSW2MkDZD0iKQXJb0g6Z9Tfw9JD0p6Jf3snvol6WpJ1ZKekzSkqbE6cJuZUZhVUuqfJtQAP46IPYHhwHhJewLnADMiYhAwI60DjAQGpVYFXNfUBRy4zcyA2qgruTUmIhZHxNy0vAp4CegHjAYmpd0mAcem5dHA5Ch4CugmqU9j13CN28yM8tS4Je0E7A3MAnpFxOK06S2gV1ruBywoOmxh6ltMA5xxm5nRvBq3pCpJc4pa1afPJ2krYCpwRkS8V7wtCp8SG/1J4YzbzIzmPTkZEROACQ1tl7QZhaB9S0T8PnUvkdQnIhanUsjS1L8IGFB0eP/U1yBn3GZmQF1Eya0xkgRMBF6KiCuKNk0DxqblscDdRf0np9klw4GVRSWVejnjNjOjRd9V8hXgJOB5Sc+mvvOAS4EpksYB84Exadt0YBRQDXwAnNLUBRy4zcygydkipYqImYAa2Dyinv0DGN+cazhwm5lBkyWQtsSB28wMv9bVzCw7zrjNzDLjjNvMLDO1UVvpIZTMgdvMjLxe6+rAbWaGv0jBzCw7zrjNzDLjWSVmZpnxrBIzs8y01CPvrcGB28wM17jNzLLjGreZWWaccZuZZcbzuM3MMuOM28wsM55VYmaWGd+cNDPLjEslZmaZ8ZOTZmaZccZtZpaZnGrcyulTZlMlqSoiJlR6HNa2+O/FpqtDpQdgJamq9ACsTfLfi02UA7eZWWYcuM3MMuPAnQfXMa0+/nuxifLNSTOzzDjjNjPLjAN3GyfpSEkvS6qWdE6lx2OVJ+nXkpZK+mulx2KV4cDdhknqCFwLjAT2BE6UtGdlR2VtwG+BIys9CKscB+62bRhQHRHzImINcDswusJjsgqLiMeAdyo9DqscB+62rR+woGh9Yeozs02YA7eZWWYcuNu2RcCAovX+qc/MNmEO3G3bbGCQpIGSOgMnANMqPCYzqzAH7jYsImqA04D7gZeAKRHxQmVHZZUm6TbgSWB3SQsljav0mKx1+clJM7PMOOM2M8uMA7eZWWYcuM3MMuPAbWaWGQduM7PMOHBbWUiqlfSspL9K+p2kLT/HuX4r6fi0fGNjL9qSdJCkAzbiGq9L6rmxYzRrTQ7cVi4fRsTgiPgSsAb4p+KNkjptzEkj4tSIeLGRXQ4Cmh24zXLiwG2t4XFg15QNPy5pGvCipI6S/l3SbEnPSfo+gAquSe8hfwjYYd2JJD0qaZ+0fKSkuZL+ImmGpJ0ofECcmbL9r0raXtLUdI3Zkr6Sjt1O0gOSXpB0I6DW/U9itvE2KusxK1XKrEcC96WuIcCXIuI1SVXAyojYV1IX4H8kPQDsDexO4R3kvYAXgV9/6rzbAzcAB6Zz9YiIdyT9Cng/Ii5L+90KXBkRMyV9gcJTqF8Efg7MjIgLJR0F+OlDy4YDt5XLFpKeTcuPAxMplDCejojXUv/hwJfX1a+BbYFBwIHAbRFRC7wp6eF6zj8ceGzduSKiofdTHwrsKa1PqLeRtFW6xjfSsX+UtGIjf0+zVufAbeXyYUQMLu5IwXN1cRfwo4i4/1P7jWrBcXQAhkfER/WMxSxLrnFbJd0P/EDSZgCSdpPUFXgM+N+pBt4HOLieY58CDpQ0MB3bI/WvArYu2u8B4EfrViSt+zB5DPhW6hsJdG+x38qszBy4rZJupFC/npu++PZ6Cv8KvAt4JW2bTOFNeJ8QEcuAKuD3kv4C3JE2/Tdw3Lqbk8DpwD7p5ueLbJjdcgGFwP8ChZLJG2X6Hc1anN8OaGaWGWfcZmaZceA2M8uMA7eZWWYcuM3MMuPAbWaWGQduM7PMOHCbmWXGgdvMLDP/H/YiRC2ZSiicAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen for X-axis – We have Predicted Values, For Y-axis – We have Actual Values. Also at diagonal, we have model correct predictions.\n",
        "\n",
        "Through the graph we can see that out of a total, 709 times the mail was ham (0) and the model predicted it right and for 410 times it was spam and model predicted spam(1), so overall we have created a good model, however one can experiment with the parameters, layers and network architecture to increase it\n",
        "\n",
        "**Classification report is also plotted similarly:**"
      ],
      "metadata": {
        "id": "QaceC8dkxskc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing classification report\n",
        "print(classification_report(y_test , y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCupM1RcybV0",
        "outputId": "abb38aa2-6bbe-4318-dc52-e92529d928db"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.85      0.84       837\n",
            "           1       0.76      0.75      0.76       544\n",
            "\n",
            "    accuracy                           0.81      1381\n",
            "   macro avg       0.80      0.80      0.80      1381\n",
            "weighted avg       0.81      0.81      0.81      1381\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here also it is evident that the model is a good one as recall and accuracy us good"
      ],
      "metadata": {
        "id": "mONAgEEGytgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting and Training Dataset(60-40 sets)\n",
        "Let's split our dataset into four subsets, X_train, X_test, y_train, and y_test."
      ],
      "metadata": {
        "id": "25CI53RvcjhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting our dataset into test and train datasets\n",
        "spam_train, spam_test = train_test_split(spam, train_size=0.6)\n",
        "ham_train, ham_test = train_test_split(ham, train_size=0.6)"
      ],
      "metadata": {
        "id": "66FaDWNQcVT5"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = ham_train.append(spam_train)\n",
        "y_train = X_train.pop('is_spam')"
      ],
      "metadata": {
        "id": "cPMETgKOcR3x"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = ham_test.append(spam_test)\n",
        "y_test = X_test.pop('is_spam')"
      ],
      "metadata": {
        "id": "P0jhP9ZRcPG1"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting our model"
      ],
      "metadata": {
        "id": "5cutLjOjqqBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting our model \n",
        "# Then, all that we have to do is initialize the Naive Bayes Classifier and fit the data. \n",
        "# For text classification problems, the Multinomial Naive Bayes Classifier is well-suited\n",
        "#"
      ],
      "metadata": {
        "id": "7tqMgl6pqoBy"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK6GWq19cMFF",
        "outputId": "e06a7b02-f970-467e-d7c3-b5532c1988f8"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes = MultinomialNB()\n",
        "spam_bayes.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL0bhF-tcJhH",
        "outputId": "e1dc57d8-7111-4f33-b9da-a4e555d837c9"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Model"
      ],
      "metadata": {
        "id": "6ligfP_Kpj1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the Model\n",
        "# Once we have put together our classifier, we can evaluate its performance in the testing set"
      ],
      "metadata": {
        "id": "KeLI43ghplUM"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes.score(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQoOWy7OcHF1",
        "outputId": "d24ec5d1-9890-42d8-ac0c-bee3c4b61eb3"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7958740499457112"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing our predicted score with our actual score to determine our model prediction power"
      ],
      "metadata": {
        "id": "1sIuD4Sjp1bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam_bayes.score(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXErwSyScEK7",
        "outputId": "bb345d93-0b4a-412b-f9c8-41f9c7afed5b"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7814425516491482"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting Confusion Matrix and Classification Reports"
      ],
      "metadata": {
        "id": "136FCXSQzH3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting y_pred by predicting over X_text and flattening it\n",
        "y_pred = spam_bayes.predict(X_test)\n",
        "y_pred = y_pred.flatten() # require to be in one-dimensional array , for easy manipulation"
      ],
      "metadata": {
        "id": "5JrJvJpfwe1Q"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing confusion maxtrix\n",
        "\n",
        "from sklearn.metrics import confusion_matrix , classification_report\n",
        "\n",
        "# creating confusion matrix \n",
        "\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "\n",
        "cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-VpmsInwl5T",
        "outputId": "6eef6e6c-2178-4e4f-ad25-831f24e201a6"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[944, 172],\n",
              "       [204, 522]])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a graph out of confusion matrix\n",
        "sns.heatmap(cm, annot = True, fmt = 'd')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "xTqWISWSxGWh",
        "outputId": "8c7506cc-4b2e-4c96-8b45-0257bacf64cb"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(33.0, 0.5, 'Actual')"
            ]
          },
          "metadata": {},
          "execution_count": 139
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEGCAYAAABFBX+4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcTUlEQVR4nO3de5xWVb3H8c9XEPCScp+DgIFJmlkieowyTSNN1ALNuyfJqNGyvL5eaWp3u9jNW2qihEOHa5rBMVIMyUsFgYok3phIBRoGRC7mJZiZ3/njWYOPMPPMMzgzz+zh++61XrP32mvvvSZ5/Vj89tprKyIwM7Ps2KnUHTAzs+Zx4DYzyxgHbjOzjHHgNjPLGAduM7OM6VzqDjRm88vLPN3FtrHLXkeUugvWDtVsWql3eo3mxJyde+/zju/3TnjEbWaWMe12xG1m1qbqakvdg6I5cJuZAdTWlLoHRXPgNjMDIupK3YWiOXCbmQHUOXCbmWWLR9xmZhnjh5NmZhnjEbeZWbZEhmaV+AUcMzPIPZwstjRB0kWSnpK0RNLFqa6npAckLU0/e6R6SbpRUqWkxZKGNXV9B24zM8ilSootBUg6EPgicBhwEHCipH2BK4A5ETEEmJP2AUYCQ1IpB25tqqsO3GZmkHs4WWwp7H3A/Ih4PSJqgIeAk4FRQEVqUwGMTtujgImRMw/oLqlfoRs4cJuZQbNG3JLKJS3MK+V5V3oKOEJSL0m7AscDA4GyiKhKbVYBZWm7P7A87/wVqa5RfjhpZgbNeuU9IsYB4xo59oyka4HZwGvAIqB2qzYhabtXQPWI28wMWvThZESMj4hDIuJIYB3wPFBdnwJJP1en5ivJjcjrDUh1jXLgNjMDImqLLk2R1Df93JtcfnsyMBMYk5qMAWak7ZnAOWl2yXBgQ15KpUFOlZiZQUu/gHO3pF7AZuCCiFgv6UfAdEljgReB01LbWeTy4JXA68C5TV3cgdvMDFp0kamI2OZTTRGxFhjRQH0AFzTn+g7cZmbgV97NzDKndnOpe1A0B24zM/B63GZmmeNUiZlZxnjEbWaWMQ7cZmbZEn44aWaWMc5xm5lljFMlZmYZ4xG3mVnGeMRtZpYxHnGbmWVMTXa+8u7AbWYGHnGbmWWOc9xmZhnjEbeZWcZ4xG1mljEecZuZZUyGZpX4K+9mZgARxZcmSLpE0hJJT0maIqmbpMGS5kuqlDRNUpfUtmvar0zHBzV1fQduMzPI5biLLQVI6g9cCBwaEQcCnYAzgGuB6yJiX2AdMDadMhZYl+qvS+0KcuA2M4MWC9xJZ2AXSZ2BXYEq4OPAXel4BTA6bY9K+6TjIySp0MUduM3MIPdwssgiqVzSwrxSvuUyESuBnwIvkQvYG4DHgPURUZ9IXwH0T9v9geXp3JrUvlehrvrhpJkZQG1t0U0jYhwwrqFjknqQG0UPBtYDvwGOa4EebuHAbWYGLTmP+xPAPyNiDYCk3wKHA90ldU6j6gHAytR+JTAQWJFSK3sCawvdwKkSMzNoyRz3S8BwSbumXPUI4GlgLnBKajMGmJG2Z6Z90vEHIwpPXfGI28wMWuwFnIiYL+ku4HGgBniCXFrl98BUSdekuvHplPHAryVVAq+Qm4FSkAO3mRkQdU3Pzy76WhHfAr61VfUy4LAG2r4JnNqc6ztwm5mB1yoxM8ucZswqKTUHbjMz8IjbzCxzMhS4PR2wHfj19N8x+n/OZ9TZ5/Hrafe87didU+7mwMNHsm79hrfV//2Z5zjoyBOYPfeRtuyqtaHbx/2Mf614kkVPzNlSN3nSrSxcMJuFC2ZT+fw8Fi6YDcAnRhzB/Hl/4InH/8j8eX/g6KMOL1W3s6sFF5lqbR5xl9jSZS9w98z7mHLH9ezceWfOv+xqPnb4h9h7wF5UVa/hL397nH5lfd92Tm1tLdfdMoGP/PewEvXa2sLEidO55ZYJTJhww5a6s87+0pbtn1z7TTZs3AjAy2tfYfRJn6Oqqpr3v38/Zt07iXcPPrTN+5xpHnFbsZa9sJwPvH8/dunWjc6dO3Ho0A/wx4f+DMCPb7yNS788lq2Xm5l810yOOepwevboXoIeW1t55NH5vLJufaPHTznlU0ydlnuHY9GiJVRVVQOwZMlz7LJLN7p06dIm/eww6qL4UmKtFrgl7S/pckk3pnK5pPe11v2yat993s3jTy5h/YaNvPHmmzzy1wWsql7Dg4/8lb59erP/kH3e1r56zcvMefgvnH7SCSXqsbUHR3z0Q1SvXkNl5T+3OXbyySfwxBNPsWnTphL0LMNqa4svJdYqgVvS5cBUQMDfUhEwRdIVBc7bsuLWHROntEbX2p33DNqbz599KuWXXMX5l36D/Ybsw6bNm7l94jS+8oXPbtP+2htu45IvfZ6ddvI/lnZkp58+mmnTZmxTf8AB7+WH37+SL11weQl6lW1RV1d0KTU18Ur89l1Ueh54f0Rs3qq+C7AkIoY0dY3NLy8r/b9HSuD6X95Jr57dub1iKt26dQVyo+w+vXsx9fbrOfu8S6n/b7Zuw0Z26dqVb11+ISOO/Egpu91mdtnriFJ3oU29+90DmPG7CoYePGJLXadOnXjphcc4bPhIVq6s2lLfv38/Hpg9nS984RL+8teFpehuydRsWllw/epivPb9c4qOObtdNfEd3++daK2Hk3XAXsCLW9X3S8csz9p16+nVoztVq1Yz56E/M2ncdXz2tNFbjh/7mTFMG38jPbrvyf133bml/qprfsbHDj9shwnalvOJEUfw3HOVbwvae+65BzNnTOTKq36wwwXtFuOPBXMxMEfSUtIC4cDewL7AV1rpnpl1yZXXsH7jRjp37sxVl32ZPd61e6m7ZO3A//76Zj525Ifp3bsnLyxbyHe++1Mm3DmV004bteWhZL0Lvnwu+75nEFdfdQlXX3UJACOPP5M1awquDmr52sFDx2K1SqoEQNJO5BZUqf/Kw0pgQUQUldnfUVMlVtiOliqx4rRIquSbZxSfKvnu1A6ZKiEi6oB5rXV9M7MW5VSJmVnGZChV4sBtZgbtYppfsRy4zczAI24zs8xx4DYzy5h28Cp7sfzetJkZuW9OFlsKkbSfpEV5ZaOkiyX1lPSApKXpZ4/UXmk9p0pJiyU1ueynA7eZGbTY6oAR8VxEDI2IocAhwOvAPcAVwJy05MectA8wEhiSSjlwa1NddeA2M4PcetzFluKNAP4RES8Co4CKVF8B1K9rMQqYGDnzgO6S+hW6qAO3mRk0a8Sdv5JpKuWNXPUMoH6p07KIqF9gZhVQlrb789bSIAAreOuN8wb54aSZGTRrVklEjAPGFWqTVkP9NPD1Bs4PSds9jcWB28wMiNoWfwFnJPB4RFSn/WpJ/SKiKqVCVqf6lcDAvPMGpLpGOVViZgat8emyM3krTQIwExiTtscAM/Lqz0mzS4YDG/JSKg3yiNvMDJqc5tccknYDjgHOy6v+ETBd0lhy3yo4LdXPAo4HKsnNQDm3qes7cJuZQYu+ORkRrwG9tqpbS26WydZtA7igOdd34DYzg0x9m8uB28wMiJrsRG4HbjMz8IjbzCxrWvLhZGtz4DYzA4+4zcyyxiNuM7Os8YjbzCxboqbUPSieA7eZGRAecZuZZYwDt5lZtnjEbWaWMQ7cZmYZE7UqdReK5sBtZoZH3GZmmRN1HnGbmWWKR9xmZhkT4RG3mVmmeMRtZpYxdRmaVeKvvJuZkXs4WWxpiqTuku6S9KykZyR9WFJPSQ9IWpp+9khtJelGSZWSFksa1tT1HbjNzGjZwA3cANwXEfsDBwHPAFcAcyJiCDAn7QOMBIakUg7c2tTFHbjNzICI4kshkvYEjgTG564bmyJiPTAKqEjNKoDRaXsUMDFy5gHdJfUrdI9Gc9ySbgIa7WJEXFi4+2Zm2dGcedySysmNjuuNi4hxaXswsAaYIOkg4DHgIqAsIqpSm1VAWdruDyzPu9aKVFdFIwo9nFxY7C9hZpZ1zZkOmIL0uEYOdwaGAV+NiPmSbuCttEj9+SFpuz+502jgjoiKxo6ZmXU0tS03q2QFsCIi5qf9u8gF7mpJ/SKiKqVCVqfjK4GBeecPSHWNajLHLamPpJ9KmiXpwfrS7F/FzKwdi1DRpfB1YhWwXNJ+qWoE8DQwExiT6sYAM9L2TOCcNLtkOLAhL6XSoGLmcU8CpgEnAOenG64p4jwzs8xo4bVKvgpMktQFWAacS26gPF3SWOBF4LTUdhZwPFAJvJ7aFlRM4O4VEeMlXRQRDwEPSVrQ/N/DzKz9amq2SPOuFYuAQxs4NKKBtgFc0JzrFxO4N6efVZJOAP4F9GzOTczM2ruOtjrgNWle4mXATcAewCWt2iszszZWW5ed11qaDNwRcW/a3AAc3brdMTMrjZZMlbS2JgO3pAk08CJORHy+VXpkZlYCdR1sWdd787a7ASeRy3ObmXUYHWo97oi4O39f0hTg0VbrkZlZCXSoVEkDhgB9W7ojW+s76NjWvoVl0KweR5S6C9ZBdahUiaRXeXuOexVweav1yMysBDrarJJ3tUVHzMxKKUOZkqLWKplTTJ2ZWZbVhYoupVZoPe5uwK5A7/SJnfre7kFurVgzsw6jo8wqOQ+4GNiL3ELg9b/VRuAXrdwvM7M2laGPvBdcj/sG4AZJX42Im9qwT2ZmbS7Izoi7mMeodZK61+9I6iHpy63YJzOzNlcTKrqUWjGB+4vpQ5cARMQ64Iut1yUzs7YXqOhSasW8gNNJktKasUjqBHRp3W6ZmbWtDpHjznMfME3SbWn/POAPrdclM7O21x5G0sUqJnBfTu4z9Oen/cXAf7Vaj8zMSqBDjbgjok7SfOA95L6R1hu4u/BZZmbZUtsRRtyS3gucmcrL5D4YTET4Ywpm1uG05JfLJL0AvArUAjURcaiknuTi6CDgBeC0iFgnScAN5D4Y/DrwuYh4vND1C80qeRb4OHBiRHw0zeWufWe/jplZ+1SHii5FOjoihkZE/UeDrwDmRMQQYE7aBxhJbtXVIeTS0rc2deFCgftkoAqYK+l2SSMgQ/+WMDNrhmhG2U6jgIq0XQGMzqufGDnzgO6S+hW6UKOBOyJ+FxFnAPsDc8m9/t5X0q2SvFi2mXUodc0oksolLcwr5VtdLoDZkh7LO1YWEVVpexVQlrb7A8vzzl1BE+tBFfNw8jVgMjA5LTZ1KrmZJrObOtfMLCvqVHxCISLGAeMKNPloRKyU1Bd4QNKzW50fkrZ78N6slcMjYl1EjIuIEdt7QzOz9qi2GaUpEbEy/VwN3AMcBlTXp0DSz9Wp+UpgYN7pA1Jdo7LzyQczs1ZUp+JLIZJ2k/Su+m3gWOApYCYwJjUbA8xI2zOBc5QzHNiQl1Jp0PZ8c9LMrMNpxmyRppQB9+Rm+dEZmBwR90laAEyXNBZ4kdx7MQCzyE0FrCQ3HfDcpm7gwG1mRst9uiwilgEHNVC/FtgmzZzWgbqgOfdw4DYzo2VfwGltDtxmZnSwtUrMzHYEtR5xm5lli0fcZmYZ48BtZpYx7eBTkkVz4DYzwyNuM7PMydKa1Q7cZmZ4HreZWeY4VWJmljEO3GZmGdNSa5W0BQduMzOc4zYzyxzPKjEzy5i6DCVLHLjNzPDDSTOzzMnOeNuB28wM8IjbzCxzapSdMbe/8m5mRi5VUmwphqROkp6QdG/aHyxpvqRKSdMkdUn1XdN+ZTo+qKlrO3CbmZFLlRRbinQR8Eze/rXAdRGxL7AOGJvqxwLrUv11qV1BDtxmZuSmAxZbmiJpAHACcEfaF/Bx4K7UpAIYnbZHpX3S8RGpfaMcuM3MaPFUyfXA13hrgN4LWB8RNWl/BdA/bfcHlgOk4xtS+0Y5cJuZ0bxUiaRySQvzSnn9dSSdCKyOiMdaq6+eVWJmBtQ2YyZ3RIwDxjVy+HDg05KOB7oBewA3AN0ldU6j6gHAytR+JTAQWCGpM7AnsLbQ/T3iNjOj5R5ORsTXI2JARAwCzgAejIizgbnAKanZGGBG2p6Z9knHH4yIgn+LOHCbmQHRjP9tp8uBSyVVksthj0/144Feqf5S4IqmLuRUiZkZrfPmZET8CfhT2l4GHNZAmzeBU5tzXQfuEuvfvx+33v4T+vTtTURQMWEqt91SQfcee/KrihvYe+8BvPTSCs4950I2rN+45byDh32A2Q/+hrGfu5iZv7uvhL+BtZYjFtxEzWtvELV1RE0t8z95Fe/95tn0OXYYdZtreP2FapZc9EtqNr5OzyM/wHuvPhN16UxsquH5707ilUeXlPpXyBSvDmhFq6mp4eqv/5DFTy5h9913Y+4jv+NPD/6Zs84+mYf/9Feu//ltXHzpeVxy6Xl8+5s/AWCnnXbi29/7GnPnPFri3ltrW3jy99j8yqtb9tc+9HeWfn8KUVvHkKvPYvCFo1l6zWQ2v/IqT3z2J/yneh277z+AYVOv5OGhXy5hz7MnO2HbOe6Sq65ew+IncyOjf//7NZ5/7h/061fGyBM+wZRJvwVgyqTfcvyJx2w5p/z8c/i/GfezZk3BB8/WAa19aDFRm/tH/YbHltJtr54AvPrUC/yneh0A/352BZ26dUFdPC5rjhqi6FJqDtztyMC9+/PBgw7gsYVP0rdvb6qr1wC54N63b28A+vUr48RPH8v42yeVsqvWJoJDpl3J8Nk/oP9nR2xztP9ZR/HynEXb1Jed+CE2/v2fxKaabY5Z49rg4WSLafO/kiWdGxETGjlWDpQD7NKlD1133qNN+1ZKu+22KxMn3czXL7+GV1/99zbH62cH/eDHV/Ptb/yYJmYLWQfwt099i/+sWkeX3ntwyPSreH3pStbNexaAwRePpq6mlqq7354u222/AQz5xlk8dtoPStHlTPOyroV9B2gwcOdPau+x+747TGTq3LkzFZNu5jfTZnLvzNkArF79MmVlfaiuXkNZWZ8taZGDDz6Q8XdeD0DPXj045pNHUVNTw6x7/1iy/lvr+M+qXOpj08sbWT1rAXscvC/r5j3LXqd/jD7HDGPhKde8rX3Xfj0ZOuEynvrKzbzxYnUpupxp7WEkXaxWCdySFjd2CChrjXtm2U23/JDnn6vkll/8akvdfbPmcObZJ3P9z2/jzLNP5g+/zwXmoQcevaXNzb+8lvvvm+ug3QF12rUrSNS+9iaddu1Kr6M+yD9+dje9jj6IQRd8igUnfYe6NzZtad95j10ZNulyll4zmfULni9hz7PLI+5ccP4kuaUL8wn4SyvdM5OGf/gQzjjrJJY89SwP/2UmAN/79s+47ue3MWHijfzPOaeyfPlKzj3nwhL31NpSlz57MnTCZQCo005U3fNn1s59ko/Ou56duuzMIdOvAnIPKJ/52ngGjv0kuw4uY5/LPsM+l30GgMdP/wGbXt7Y6D3s7WozlH5Ua+RKJY0HJkTENvPVJE2OiLOausaOlCqx4k3b7dBSd8HaoWOrpxZcBrUYZ737pKJjzuQX73nH93snWmXEHRFjCxxrMmibmbW1HT7HbWaWNc5xm5lljF95NzPLGKdKzMwyJkuzShy4zcxwqsTMLHP8cNLMLGOc4zYzyxinSszMMiZLK246cJuZAbUZGnH7QwpmZuRSJcWWQiR1k/Q3SU9KWiLpO6l+sKT5kiolTZPUJdV3TfuV6figpvrqwG1mRi5VUmxpwn+Aj0fEQcBQ4DhJw4FrgesiYl9yK6fWr+k0FliX6q9L7Qpy4DYzo+VG3JFT/xmrnVMJ4OPAXam+AhidtkelfdLxEZIKrj7owG1mRvO+OSmpXNLCvFKefy1JnSQtAlYDDwD/ANZHRP2HQFcA/dN2f2A5QDq+AehVqK9+OGlmRvNeec//zGIjx2uBoZK6A/cA+7/jDubxiNvMjJZLleSLiPXAXODDQHdJ9YPlAcDKtL0SGAiQju8JrC10XQduMzNadFZJnzTSRtIuwDHAM+QC+Cmp2RhgRtqemfZJxx+MJp6AOlViZkaLvoDTD6iQ1Inc4Hh6RNwr6WlgqqRrgCeA8an9eODXkiqBV4AzmrqBA7eZGS33yntELAYObqB+GXBYA/VvAqc25x4O3GZmeJEpM7PMqY3sLOzqwG1mhheZMjPLHC/ramaWMc5xm5llTJ1TJWZm2eIRt5lZxnhWiZlZxjhVYmaWMU6VmJlljEfcZmYZ4xG3mVnG1EZtqbtQNAduMzP8yruZWeb4lXczs4zxiNvMLGM8q8TMLGM8q8TMLGOy9Mq7v/JuZkYux11sKUTSQElzJT0taYmki1J9T0kPSFqafvZI9ZJ0o6RKSYslDWuqrw7cZmbkctzFlibUAJdFxAHAcOACSQcAVwBzImIIMCftA4wEhqRSDtza1A0cuM3MaLkRd0RURcTjaftV4BmgPzAKqEjNKoDRaXsUMDFy5gHdJfUrdA8HbjMzcvO4iy2SyiUtzCvlDV1T0iDgYGA+UBYRVenQKqAsbfcHluedtiLVNcoPJ83MaN487ogYB4wr1EbS7sDdwMURsVFS/vkhabunsThwm5nRsrNKJO1MLmhPiojfpupqSf0ioiqlQlan+pXAwLzTB6S6RjlVYmZGyz2cVG5oPR54JiJ+nndoJjAmbY8BZuTVn5NmlwwHNuSlVBrkEbeZGS36yvvhwGeBv0talOquBH4ETJc0FngROC0dmwUcD1QCrwPnNnUDB24zM1ruzcmIeBRQI4dHNNA+gAuacw8HbjMzvMiUmVnmZGmRKWXpb5kdlaTyNP3IbAv/udhxeVZJNjQ4ud92eP5zsYNy4DYzyxgHbjOzjHHgzgbnMa0h/nOxg/LDSTOzjPGI28wsYxy4zcwyxoG7nZN0nKTn0meNrmj6DOvoJP1K0mpJT5W6L1YaDtztmKROwM3kPm10AHBm+gSS7djuBI4rdSesdBy427fDgMqIWBYRm4Cp5D5zZDuwiHgYeKXU/bDSceBu35r9SSMz6/gcuM3MMsaBu31r9ieNzKzjc+Bu3xYAQyQNltQFOIPcZ47MbAfmwN2ORUQN8BXgfuAZYHpELCltr6zUJE0B/grsJ2lF+hSW7UD8yruZWcZ4xG1mljEO3GZmGePAbWaWMQ7cZmYZ48BtZpYxDtzWKiTVSlok6SlJv5G06zu41p2STknbdxRaaEvSUZI+sh33eEFS7+3to1lbcuC21vJGRAyNiAOBTcD5+Qcldd6ei0bEFyLi6QJNjgKaHbjNssSB29rCI8C+aTT8iKSZwNOSOkn6iaQFkhZLOg9AOb9I65D/EehbfyFJf5J0aNo+TtLjkp6UNEfSIHJ/QVySRvtHSOoj6e50jwWSDk/n9pI0W9ISSXcAatv/S8y233aNesyKlUbWI4H7UtUw4MCI+KekcmBDRPy3pK7AnyXNBg4G9iO3BnkZ8DTwq62u2we4HTgyXatnRLwi6ZfAvyPip6ndZOC6iHhU0t7k3kJ9H/At4NGI+K6kEwC/fWiZ4cBtrWUXSYvS9iPAeHIpjL9FxD9T/bHAB+vz18CewBDgSGBKRNQC/5L0YAPXHw48XH+tiGhsfepPAAdIWwbUe0jaPd3j5HTu7yWt287f06zNOXBba3kjIobmV6Tg+Vp+FfDViLh/q3bHt2A/dgKGR8SbDfTFLJOc47ZSuh/4kqSdASS9V9JuwMPA6SkH3g84uoFz5wFHShqczu2Z6l8F3pXXbjbw1fodSfV/mTwMnJXqRgI9Wuy3MmtlDtxWSneQy18/nj58exu5fwXeAyxNxyaSWwnvbSJiDVAO/FbSk8C0dOj/gJPqH04CFwKHpoefT/PW7JbvkAv8S8ilTF5qpd/RrMV5dUAzs4zxiNvMLGMcuM3MMsaB28wsYxy4zcwyxoHbzCxjHLjNzDLGgdvMLGP+H2DNIO49DTugAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen for X-axis – We have Predicted Values, For Y-axis – We have Actual Values. Also at diagonal, we have model correct predictions.\n",
        "\n",
        "Through the graph we can see that out of a total, 944 times the mail was ham (0) and the model predicted it right and for 522 times it was spam and model predicted spam(1), so overall we have created a good model, however one can experiment with the parameters, layers and network architecture to increase it\n",
        "\n",
        "**Classification report is also plotted similarly:**"
      ],
      "metadata": {
        "id": "5hAUG4j0xiXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# printing classification report\n",
        "print(classification_report(y_test , y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd4Y73LFyUto",
        "outputId": "9921c3ef-b603-4c60-ec04-f0563b40d638"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.85      0.83      1116\n",
            "           1       0.75      0.72      0.74       726\n",
            "\n",
            "    accuracy                           0.80      1842\n",
            "   macro avg       0.79      0.78      0.78      1842\n",
            "weighted avg       0.79      0.80      0.80      1842\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here also it is evident that the model is a good one as recall and accuracy us good"
      ],
      "metadata": {
        "id": "hYU7xiNsyw5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "From the three splits, we see that model-score decreases as the splitting size increases\n",
        "\n",
        "We also notice that recall and f1-score remains the same for all the splits\n",
        "\n",
        "Found the naive bayes provides max accuracy of 80%.\n",
        "\n",
        "Found high precision and recall score of 0.82 and 0.85 respectively.\n",
        "\n",
        "Confusion matrix shows high classification accuracy with only a few values incorrect.\n",
        "\n",
        "Overall model fit is good."
      ],
      "metadata": {
        "id": "lAf5-CjO2Tjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenging the solution\n",
        "\n",
        "We challenge our solution by performing the mse of the predicted values"
      ],
      "metadata": {
        "id": "DxNyqtbQ3EVy"
      }
    }
  ]
}